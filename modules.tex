\documentclass[12pt]{article}

\usepackage{amssymb,amsmath,amsthm}

% theorem styles (I like everything to have the same counter)
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{exercise}[theorem]{Exercise}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% some numbering settings
\numberwithin{equation}{section}

\usepackage{fancyhdr}
\usepackage{lastpage}
\setlength{\headheight}{15.2pt}
\renewcommand{\footrulewidth}{0.4pt}% default is 0pt
\setlength{\footskip}{30pt}
\pagestyle{fancy}

\makeatletter
\let\ps@plain\ps@fancy 
\makeatother

\lhead{MATH 742}
\chead{Modules and Wedderburn Theory}
\rhead{Spring 2023}
\lfoot{Last Revised: \today}
\cfoot{}
\rfoot{\thepage\ of \pageref{LastPage} }

\begin{document}

%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%
\title{Modules and Wedderburn Theory}
\author{Alexander Duncan}

\maketitle

Modern representation theory is usually phrased in the language of
non-commutative rings/algebras and their modules.
Until now I've avoided this language so I could focus on the basics.
However, the module-theoretic perspective is much more versatile --- even
for finite groups over $\mathbb{C}$.
It is much more urgently needed to discuss representation
theory over other fields. 

The main object is the \emph{group ring} $\mathbb{Z}G$
or \emph{group algebra} $kG$.  We will see that most of the language of
representation theory of finite groups can be efficiently expressed
as special cases of module theory over $kG$.
Then we will use the language to go even further.

Since module theory is part of the standard algebra qual sequence,
I will frequently omit proofs and even many definitions under the
assumption that you should have seen them before.
However, the emphasis in qual courses is usually on
\emph{commutative} rings so some reminders are in order.
I recommend finding a graduate algebra textbook with a good treatment of
modules, such as \cite[\S{10}]{DF} or \cite[\S{III}]{Lang},
to refer to if something is unfamiliar.

The ``basics'' of module-theoretic representation theory is in
almost every text I've referenced, often at a very early stage; 
see \cite{DF}, \cite{Etingof}, \cite{FultonHarris}, \cite{Lang} etc.
Serre's book \cite{Serre}, does not use the module-theoretic
perspective at first, but then abruptly changes gears in chapter 6 and
assumes you've seen Wedderburn theory (developed below).
I will also draw heavily from \cite[\S{12,13}]{AlperinBell},
\cite{AlperinLRT}, and \cite{CurtisReiner}.

\subsection*{Conventions}

Throughout, we assume that all rings and algebras are unital (have an identity
element) but are \emph{not necessarily} commutative.
We will reserve $k$ for a commutative ring, which will usually be a
field or $\mathbb{Z}$.
When a ring $R$ is commutative, we will typically ignore the
distinction between left and right $R$-modules and
simply call them $R$-modules.

%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%

\section{Warm up: Matrix Algebra over a Field}

In this section, we consider the particular case of the non-commutative
$k$-algebra $\operatorname{M}_n(k)$ where $k$ is a field.
Many of the quirks of non-commutative rings will show themselves even in this
setting.

\begin{example}
The space $k^n$ of column vectors is a left $\operatorname{M}_n(k)$-module.
The space $(k^n)^T$ of row vectors is a right $\operatorname{M}_n(k)$-module.
More generally, for an integer $m$, the space of $n \times m$-matrices
$\operatorname{M}_{n \times m}(k)$ is a left $\operatorname{M}_n(k)$-module
and the space of $m \times n$-matrices
$\operatorname{M}_{m \times n}(k)$ is a right $\operatorname{M}_n(k)$-module.
\end{example}

We will see shortly that the above example has
exhibited all isomorphism classes of finitely-generated
left and right $\operatorname{M}_{m \times n}(k)$-modules.

\begin{definition}
Let $R$ be a ring and $M$ a (left or right) $R$-module.
We say $M$ is \emph{simple} if $M\ne 0$ has no submodules except for
$0$ and $M$ itself.
We say $M$ is \emph{indecomposable} if $M$ cannot be written as a direct sum of
non-zero submodules.
We say $M$ is \emph{semisimple} if $M$ is a direct sum of simple
submodules.
\end{definition}

From the above example, we see that $k^n$ is both simple and
indecomposable as a left $\operatorname{M}_n(k)$-module.
We have a decomposition
\[
\operatorname{M}_{n \times m}(k) \cong (k^n)^{\oplus m}
\]
as left $\operatorname{M}_n(k)$-modules,
so $\operatorname{M}_{n \times m}(k)$ is not simple or
indecomposable; however, it is semisimple.

\subsection{Ideals}

Given a subspace $V \subseteq k^n$, let
$I$ be the subset of $\operatorname{M}_n(k)$
where every row is taken from $V$,
and let $J$ be the subset of $\operatorname{M}_n(k)$
where every column is taken from $V$.
Observe that multiplication of $I$ on the left takes each row to a
linear combination of elements of $V$, so we conclude $I$ is a left
ideal.  Similarly, $J$ is a right ideal.

This procedure is invertible.
Given a left ideal $I$ of $\operatorname{M}_n(k)$,
we can reconstruct the subspace $V$
as the set of rows occurring among the elements of $I$.
Similarly, for right ideals.  We conclude:

\begin{proposition}
The left (resp. right) ideals of $\operatorname{M}_n(k)$ are
in canonical bijective correspondence with the vector subspaces of $k^n$.
\end{proposition}

\begin{example}
I find the correspondences are somehow easier to understand
via specific examples.  Let $V$ be the subspace,
$I$ the corresponding left ideal, and $J$ the corresponding right ideal.
The values $a,b,c,d,e,f$ represent arbitrary elements of $k$.
In $\operatorname{M}_2(k)$, we have the example:
\[
V = \operatorname{span}_k \left\{
\begin{pmatrix} 1 \\ 0 \end{pmatrix} \right\}
\quad
I = \left\{ \begin{pmatrix} a & 0 \\ b & 0 \end{pmatrix} \right\} 
\quad
J = \left\{ \begin{pmatrix} a & b \\ 0 & 0 \end{pmatrix} \right\}
\]
In $\operatorname{M}_3(k)$, we have
\[
V = \operatorname{span}_k \left\{
\begin{pmatrix} 1 \\ 1 \\ 0 \end{pmatrix},
\begin{pmatrix} 0 \\ 1 \\ 1 \end{pmatrix} \right\}
\quad
J = \left\{ \begin{pmatrix}
a & b & c \\
a+d & b+e & c+f \\
d & e & f
\end{pmatrix} \right\}
\]
and $I$ is just the transpose.
\end{example}

\begin{remark}
For those who know some algebraic geometry,
the set of subspaces of fixed dimension $r$ in $k^n$ is the
\emph{Grassmannian} $\operatorname{Gr}(n,r)$.
In particular, the set of subspaces of dimension $1$ is the projective
space $\mathbb{P}^{n-1}$.
The correspondence above allows us to define Grassmannians and
projective spaces as sets of left ideals of $\operatorname{M}_n(k)$.
This shift of perspective is useful when we define more general objects
such as Severi-Brauer varieties when the algebra is not a matrix
algebra.
\end{remark}

Choosing a basis $v_1,\ldots v_m$ for a subspace $V$, consider the matrix
\[
M_V := \begin{pmatrix}
\vdots & \cdots & \vdots & \vdots & \cdots & \vdots\\
v_1 & \cdots & v_n & 0 & \cdots & 0\\
\vdots & \cdots & \vdots & \vdots & \cdots & \vdots\\
\end{pmatrix} 
\]
whose first $m$ columns are the basis vectors $v_1,\ldots,v_m$ and the
remaining columns are $0$.
We observe that the corresponding right ideal $J$ is generated by $M_V$.
Similarly, $I$ is generated by $M_V^T$.

Recall that an ideal is \emph{principal}
if it is generated by one element.
Thus we have the following:

\begin{corollary}
Every left (resp. right) ideal of $\operatorname{M}_n(k)$ is principal.
\end{corollary}

\begin{definition}
We say a ring $R$ is \emph{simple} if its only two-sided ideals are $0$ and
$R$ itself.
We say a ring $R$ is \emph{semisimple} if it is a product of
simple rings.
\end{definition}

From the explicit description of left and right ideals
of $\operatorname{M}_n(k)$, we see that:

\begin{proposition}
$\operatorname{M}_n(k)$ is a simple ring.
\end{proposition}

Be careful about a possible ambiguity with regard to the word
``simple.''
Recall that $\operatorname{M}_n(k)$ can also be viewed a left (or right)
module over itself.
As a left module,
$\operatorname{M}_n(k)$ is usually \emph{not} simple since it has many
submodules (left ideals).

We will see later that a finite-dimensional algebra is semisimple as a ring if
and only if it is semisimple as a (left or right) module,
but this is not obvious: the simple summands are not the same in each
context!

\subsection{Idempotents}

\begin{definition}
Let $R$ be a ring.  An element $e$ is an \emph{idempotent} if $e^2=e$.
Two idempotents $e_1,e_2$ are \emph{orthogonal} if $e_1e_2=e_2e_1=0$.
A \emph{primitive idempotent} is a non-zero idempotent
that is not a sum of two non-zero orthogonal idempotents.
\end{definition}

The idempotents of $\operatorname{M}_n(k)$ are exactly the (matrices
representing) the projections $\pi : k^n \to k^n$.
Given an idempotent $e \in \operatorname{M}_n(k)$ with corresponding
projection $\pi_e$, we obtain a direct sum decomposition
\[
k^n = \ker(\pi_e) \oplus \operatorname{im}(\pi_e)
\ .
\]
Observe that idempotent $1-e$ is orthogonal to $e$ and that
\[
\ker(\pi_{1-e}) = \operatorname{im}(\pi_{e})
\textrm{ and }
\operatorname{im}(\pi_{1-e}) = \ker(\pi_e).
\]
Thus, we have a decomposition of vector spaces
\[
k^n = e(k^n) + (1-e)(k^n)
\]
where we warn that $e(k^n)$ and $(1-e)(k^n)$ are \emph{not}
$\operatorname{M}_n(k)$-modules.

More generally, a set of orthogonal idempotents $e_1,\ldots,e_r$
such that $1=e_1+\ldots+e_r$ gives a decomposition
\[
k^n = V_1 \oplus V_2 \oplus \cdots \oplus V_r
\]
where $V_i = \operatorname{im}(\pi_i) = e_i(k^n)$ for $1 \le i \le r$.

We see that the primitive idempotents of $\operatorname{M}_n(k)$
correspond to the rank $1$ projections.
Moreover, a maximal set $e_1,\ldots,e_r$ of orthogonal primitive
idempotents amounts to a choice of basis for $k^n$.

\begin{example}
Consider the matrix algebra $\operatorname{M}_2(\mathbb{C})$
and set
\[
a = \begin{pmatrix} 1 & 0 \\ 0 & 0 \end{pmatrix},
b = \begin{pmatrix} 0 & 0 \\ 0 & 1 \end{pmatrix},
c = \begin{pmatrix} \frac{1}{2} & \frac{1}{2} \\ \frac{1}{2} & \frac{1}{2} \end{pmatrix},
\textrm{ and }
d = \begin{pmatrix} \frac{1}{2} & -\frac{1}{2} \\ -\frac{1}{2} & \frac{1}{2} \end{pmatrix}.
\]
Each of $a,b,c,d$ are primitive idempotents.
We see that $a,b$ are orthogonal and $c,d$ are orthogonal.
However, $a,c$ are not orthogonal.
\end{example}

Idempotents are a handy language for understanding
decompositions of ideals.
If $V$ is the subspace corresponding to a left ideal $I$,
then $I = \operatorname{M}_n(k)e$ where $e$ is the idempotent
corresponding to a projection $k^n \to V$.

More generally, if $e_1,\ldots,e_r$ is a set of orthogonal
idempotents satisfying $e_1+\cdots+e_r=1$, then
\[
\operatorname{M}_n(k) = \operatorname{M}_n(k)e_1
\oplus \cdots \oplus \operatorname{M}_n(k)e_r
\]
as left $\operatorname{M}_n(k)$-modules and
\[
\operatorname{M}_n(k) = e_1\operatorname{M}_n(k)
\oplus \cdots \oplus e_r\operatorname{M}_n(k)
\]
as right $\operatorname{M}_n(k)$-modules.
Moreover, we have the decomposition
\[
\operatorname{M}_n(k) = \bigoplus_{1\le i,j \le r}
e_i\operatorname{M}_n(k)e_j
\]
as $k$-vector spaces, which can be thought of as a kind of block matrix
decomposition.

\begin{definition}
Let $R$ be a ring.
The \emph{center} of $R$ is the subset
\[
Z(R) := \{ z \in R \mid rz=zr \textrm{ for all } r \in R \},
\]
which is a commutative subring of $R$.
\end{definition}

Note that $Z(\operatorname{M}_n(k)) \cong k$ is just the set of scalar
matrices.

\begin{definition}
An idempotent $e$ is \emph{central} if $e$ is contained in the center
$Z(R)$ of $R$.
A \emph{primitive central idempotent} is a non-zero central idempotent
that is not a sum of two non-zero orthogonal central idempotents.
\end{definition}

Warning: a primitive central idempotent is always central
but not necessarily primitive!
Sometimes the term ``centrally primitive'' is used
instead of ``primitive central,'' which is either more or less confusing
depending on which side of the bed you rolled out of that morning.

The matrix algebra $\operatorname{M}_n(k)$ has exactly two central
idempotents: the zero matrix and the identity matrix.
Only the identity matrix is a primitive central idempotent.
(The identity is \emph{not} a primitive idempotent unless $n=1$.)

Central idempotents are not so interesting for the matrix ring, but they
are interesting for products of rings.

\begin{exercise}
Let $R$ be a ring and suppose $e$ is a central idempotent.
Then $eRe$ is a ring under the same addition and multiplication
where $e$ is the identity.
(Note $eRe$ is a subring only $eRe$ when it contains the
identity, which only happens if $e=1$.)
\end{exercise}

\begin{exercise}
Let $R$ be a ring.  A decomposition
\[
R= R_1 \times \cdots \times R_n
\]
where $R_1,\ldots,R_n$ are rings corresponds to a set of orthogonal
central idempotents $e_1,\ldots,e_n$ such
that $R_i \cong e_iRe_i$ for each $1 \le i \le n$.
\end{exercise}

\begin{exercise}
Let $R$ be a ring and suppose $e$ is a primitive central idempotent.
Then $eRe$ cannot be written as a product of two non-zero rings.
\end{exercise}

%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%


\section{The Group Ring}

\begin{definition}
Suppose $G$ is a finite group.
The \emph{group ring} $\mathbb{Z}G$ is the free abelian group
on $G$ with multiplication given by
\[
\left( \sum_{g \in G} a_g g \right) \cdot
\left( \sum_{g \in G} b_g g \right)
:= \sum_{g \in G} \sum_{h \in G} a_gb_h gh
\]
for integers $\{a_g\}_{g\in G}$ and $\{b_g\}_{g\in G}$.
More generally, given a commutative ring $k$,
the \emph{group algebra} $kG$ is the free $k$-module on $G$
with multiplication as above.
\end{definition}

One checks immediately that $\mathbb{Z}G$ is a non-commutative ring with
identity $1$ corresponding to the basis element of the identity in $G$.
Similarly, $kG$ is a $k$-algebra with identity $1$.
Since $1 \in G$ corresponds to $1 \in \mathbb{Z}G$,
the identification of the basis of $\mathbb{Z}G$ with the elements of
$G$ is mostly harmless if $G$ is written multiplicatively.

\begin{example}
Suppose $G=D_6=\langle s,r \mid s^2, r^3, (sr)^2 \rangle$
is the dihedral group of order $6$.
We have the following computation in $\mathbb{Z}G$:
\begin{align*}
&(3 + 2s + 5r)(r-sr + 5r^2)\\
=& 3r -3sr +15r^2 + 2sr -2r+10sr^2 +5r^2 -5s+25\\
=& 25 -5s + r + 20r^2 -sr +10sr^2
\end{align*}
\end{example}

\begin{example}
Let $G = \langle r \mid r^n \rangle$ be a cyclic group of order $n$.
Thus $G$ has basis $\{1,r,r^2,\ldots, r^{n-1} \}$
where $r^i\cdot r^j=r^{i+j}$ subject to the relation $r^n=1$.
In other words, $\mathbb{Z}G$ is isomorphic to the ring
$\mathbb{Z}[x]/(x^n-1)$.
\end{example}

Note that using additive notation $\mathbb{Z}/n\mathbb{Z}$
for the cyclic group of order $n$ would be dangerously
ambiguous in the example above.
For this reason, we usually use ``exponential notation''
for group rings of additive groups.
If $a+b=c$ in an additive group, then
we write $x^ax^b=x^{a+b}=x^c$ in the group ring.

\begin{exercise}
Prove that $\mathbb{Z}G$ is commutative if and only $G$ is abelian.
\end{exercise}

It is useful to write the coefficients of the product as a formula of
the multiplicands.  Specifically, if
\[
\left( \sum_{g \in G} a_g g \right) \cdot
\left( \sum_{g \in G} b_g g \right)
= \sum_{g \in G} c_g g
\]
then
\[
c_g = \sum_{h \in G} a_hb_{h^{-1}g}
\]
for every $g \in G$.

Since $G$ is finite, an equivalent description of $\mathbb{Z}G$ is as the set
$\operatorname{Hom}_{\mathrm{Set}}(G,\mathbb{Z})$
of functions from $G$ to $\mathbb{Z}$.
However, the ring structure is \emph{not} the ``obvious one.''
The additive structure is indeed pointwise addition,
but we use the \emph{convolution product}
of two functions $f_1,f_2$ defined via
\[
(f_1 \ast f_2)(g) = \sum_{ij=g} f_1(i)f_2(j)
\]
for $g \in G$ and the indices in the sum $i,j$
vary over all pairs of elements of $G$ such that $ij=g$.
The isomorphism with $\mathbb{Z}G$ is by identifying each basis element
$g \in \mathbb{Z}G$ with the characteristic function $\chi_g$
in $\operatorname{Hom}_{\mathrm{Set}}(G,\mathbb{Z})$
satisfying $\chi_g(h) = \delta_{gh}$ for all $h \in G$.

\subsection{Modules of the Group Ring}

The relevance of the group ring to group theory is evident from the
following proposition:

\begin{proposition} \label{prop:equivalence}
Let $G$ be a finite group and $k$ a field.
The structure of a representation of $G$ over the field $k$
is equivalent to the structure of a left $kG$-module.
Finite-dimensional representations correspond to
finitely generated modules.
Moreover, $G$-equivariant linear transformations are
exactly the $kG$-module homomorphisms.
\end{proposition}

\begin{proof}
Let $M$ be a left $kG$-module.
In particular, $M$ is a $k$-vector space.
We construct a linear representation $\rho : G \to \operatorname{GL}(M)$
via $\rho_g(m)=gm$ for $g \in G$ and $m\in M$.
Conversely, given a linear representation $\sigma : G \to
\operatorname{GL}(V)$, we endow $V$ with a left $kG$-module structure via
\[
\left( \sum_{g \in G} a_g g \right) v := \sum_{g \in G} a_g \sigma_g(v)
\]
for $\{a_g\}$ from $k$ and $v \in V$.
These are mutually inverse operations.

If $M$ is a finitely-generated left $kG$-module,
say by $\{s_1,\ldots,s_n\}$, then $M$ is spanned as a $k$-vector space
by $\{ gs_i \mid g \in G, 1 \le i \le n \}$.
Thus $\rho$ is a finite-dimensional representation.
Conversely, if $V$ is finite-dimensional, then a basis for $V$
as a $k$-vector space is a fortiori a generating set for $V$ as a
$kG$-module.

Finally, we observe that a $G$-equivariant linear transformation
$f : (V,\rho) \to (W,\sigma)$ of representations are exactly the $kG$-module
homomorphisms under the above correspondence.
Indeed,
\begin{align*}
&f\left( \left( \sum_{g\in G} c_g g \right) v \right)\\
=& f\left( \sum_{g\in G} c_g \rho_g(v) \right)\\
=& \sum_{g\in G} c_g f\left( \rho_g(v) \right)\\
=& \sum_{g\in G} c_g \sigma_g\left(f(v)\right)\\
=& \left(\sum_{g\in G} c_g g\right) f(v)
\end{align*}
where we used the fact that $f$ is linear and $G$-equivariant.
\end{proof}

Immediately, we see that subrepresentations correspond to submodules,
quotient representations correspond to quotient modules, and direct sums
correspond to direct sums.
However, while $V \otimes_k W$ and $\operatorname{Hom}_k(V,W)$ have
induced $kG$-module structures, they are \emph{not} the same as the
module-theoretic constructions $V \otimes_{kG} W$
and $\operatorname{Hom}_{kG}(V,W)$ discussed below.
Moreover, the ``trivial module'' is the zero representation,
\emph{not} the ``trivial representation''.

\begin{example}
Every ring is a both a left and right module over itself.
The group algebra $kG$ viewed as a left $kG$-module corresponds to
the regular representation of $G$.
\end{example}

Another important subtlety in the above is that the dimension of a
representation is \emph{not} in general the same as the minimal number
of generators of the corresponding $kG$-module.

A left $kG$-module is indecomposable if it is indecomposable as a
representation.
A left $kG$-module is simple if and only if it is irreducible
as a representation of $G$.
In particular, irreducible representations can be
generated by one element as a left $kG$-module (though the converse may not
be true).

\begin{theorem}[Maschke's Theorem restated]
If $G$ is finite of order coprime to the characteristic of a field $k$,
then every finitely-generated left $kG$-module is semisimple.
\end{theorem}

\subsection{Center of the Group Ring}

The center of a group ring is of special importance.
Note that we also have the notion of the center $Z(G)$ of a group $G$.
Indeed, the group ring of the center of a group is contained in the
center of the group ring; in other words
\[
kZ(G) \subseteq Z(kG).
\]
However, equality holds only when $G=Z(G)$, which is a consequence of the
following result.

\begin{proposition}
The center $Z(kG)$ of the group algebra $kG$ has basis
\[
\left\{
\sum_{g \in K} g\ \middle|\ K \in \mathcal{K}
\right\}
\]
where $\mathcal{K}$ is the set of conjugacy classes of $G$.
\end{proposition}

\begin{proof}
Let $K$ be a conjugacy class in $\mathcal{K}$.
Note that $g \in K$ implies
$hgh^{-1} \in K$.
Thus we observe that
\[
h\left(\sum_{g \in K} g\right)
=\left(\sum_{g \in K} g\right) h
\]
for all $h \in H$. 
Conversely, any element $x$ of the group algebra
satisfying the property $hx=xh$ must have the same coefficient
on basis elements belonging to the same conjugacy class.
\end{proof}

\begin{example}
Suppose $G=D_6=\langle s,r \mid s^2, r^3, (sr)^2 \rangle$
is the dihedral group of order $6$.
We determine that
$Z(\mathbb{Z}G)$ has basis
\[
\{ 1, a = s+sr+sr^2, b = r+r^2 \}.
\]
The multiplication table is determined by a few calculations:
\begin{align*}
a^2 &= 3+3b\\
ab=ba &= 2a\\
b^2 &=2+b.
\end{align*}
Thus, the structure of the center is a bit obscure in this basis.
\end{example}

When $k=\mathbb{C}$ the center is especially easy to describe.
We will see in Corollary~\ref{cor:CGcenter} below that
$Z(\mathbb{C}G) \cong \mathbb{C}^{|K|}$ as $\mathbb{C}$-algebras.
However, this is not at all obvious in the
basis described above!

\subsection{Decomposition of the Complex Group Algebra}

Let $G$ be a finite group.
Let $W_1,\ldots,W_r$ be the distinct
complex irreducible representations of $G$
and let $n_1,\ldots, n_r$ be their dimensions.

\begin{theorem}
There is a canonical isomorphism
\[
\mathbb{C}G \cong \bigoplus_{i=1}^r \operatorname{End}_{\mathbb{C}}(W_i)
\]
of $\mathbb{C}$-algebras.
\end{theorem}

\begin{proof}
If $V$ is a left $\mathbb{C}G$-module and $x$ is an element of $kG$,
then the multiplication map $v \mapsto xv$ is an endomorphism
of $V$.
This gives a map from $\mathbb{C}G$ to each
$\operatorname{End}_C(W_i)$ and we obtain
a map to the direct sum.
The map is injective since the action on the regular representation
is faithful.
Since both algebras have dimension $n_1^2+\cdots+n_r^2$,
this must be an isomorphism.
\end{proof}

Note that for any complex vector space $V$ of dimension $n$, we have
a (non-canonical) isomorphism
$\operatorname{End}(V) \cong \operatorname{M}_n(\mathbb{C})$.
Thus, we can rewrite the theorem above as:  

\begin{corollary} \label{cor:fourier_as_matrices}
$\displaystyle
\mathbb{C}G \cong \bigoplus_{i=1}^r \operatorname{M}_{n_i}(\mathbb{C})$
\end{corollary}

Recall that the center of a matrix algebra
$\operatorname{M}_n(\mathbb{C})$ is just the subalgebra of scalar
matrices, which is isomorphic to $\mathbb{C}$.
In view of Corollary~\ref{cor:fourier_as_matrices},
therefore have:

\begin{corollary} \label{cor:CGcenter}
$\displaystyle Z(\mathbb{C}G) \cong \mathbb{C}^r$
as $\mathbb{C}$-algebras.
\end{corollary}

Thus we have two bases of $Z(\mathbb{C}G)$: one indexed by conjugacy
classes and one indexed by irreducible representations.
The character table of $G$ is exactly the change of basis matrix between
these two bases.

\begin{example}
Let $G = \mathbb{Z}/3\mathbb{Z}$ be the cyclic group of order $3$.
Note $kG \cong k[x]/(x^3-1)$ for any field $k$.
We find that $\mathbb{R}G \cong \mathbb{R} \oplus \mathbb{C}$,
so the corollary is more subtle over non-closed fields.
Moreover, the element $x-1$ in $\mathbb{F}_3G$ is nilpotent,
so things are potentially much worse when Maschke's theorem does not
hold.
\end{example}

\subsection{Idempotents of the Group Ring}

Earlier in the course, we have seen several examples of projections in
$\operatorname{End}_k^G(V)$ for various representations $(V,\rho)$.
For example, the projection $\pi : V \to V^G$ onto the invariant
subspace has the formula
\[
\pi(v) = \frac{1}{|G|} \sum_{g \in G} \rho_g(v)
\]
for $v \in V$.
Other examples include the Young projectors, Young symmetrizers,
and the projections onto isotypic components.
These are all more naturally considered as coming from elements
of the group algebra.

Given a representation $\rho : G \to \operatorname{GL}(V)$
and an element
\[
f = \sum_{g \in G} c_g g
\]
in the group algebra $kG$,
define $\widehat{f}(\rho) \in \operatorname{End}(V)$ via
\[
\widehat{f}(\rho)(v) = \sum_{g \in G} c_g \rho_g(v)
\]
for all $v \in V$.
Thus, the projection $\pi$ above is simply the endomorphism
$\widehat{f}(\rho)$
where $f = |G|^{-1} \sum_{g \in G} g$.
In practice, we will simply write $f$ instead of $\widehat{f}(\rho)$
since there is rarely danger of confusion.

First, observe that if $p \in kG$ is an idempotent,
then $\widehat{p}(\rho)$ is a projection for any representation $\rho$.
(The converse is not true, consider the direct sum of two copies the regular
representation and the projection onto a summand.)
Second, if $p$ is a primitive idempotent, then the corresponding
projection $\pi$ on the regular representation has image an
indecomposable subrepresentation.

For the complex group ring, we can even say more.

\begin{proposition}
Let $\chi_1,\ldots \chi_r$ be the irreducible complex characters of $G$.
The primitive central idempotents of $\mathbb{C}G$ are precisely
the elements
\[
e_i := \frac{\chi_i(1)}{|G|} \sum_{g\in G} \overline{\chi_i(g)} g
\]
for each $i=1,\ldots,r$.
If $(V,\rho)$ is a representation of $G$, then
$\widehat{e_i}(\rho) : V \to V$ is the projection onto the isotypic
component of $V$ corresponding to $\chi_i$.
\end{proposition}

\begin{proof}
From the isomorphism
\[
\mathbb{C}G \cong \bigoplus_{i=1}^r \operatorname{End}(W_i)
\]
we recall that the center is $\mathbb{C}^r$,
corresponding to scalar multiplication in each
$\operatorname{End}(W_i)$.
The central idempotents of $\mathbb{C}G$
correspond to the case where each scalar multiplication is $1$ or $0$.
The primitive central idempotents are those which are the identity
in exactly one $\operatorname{End}(W_i)$ and trivial elsewhere.
The explicit formulas for the primitive central idempotents now follow
from the explicit formula for the projection onto the isotypic
components of a representation. 
\end{proof}

\section{Bimodules and Tensor Products}

Recall $R$ is a commutative ring, we will often just say $R$-module without
specifying whether it is left or right.

\begin{definition}
If $k$ is a \emph{commutative} ring, then we define an
\emph{$k$-algebra} as a (not-necessarily commutative) ring $R$ along
with a ring homomorphism $\pi : k \to R$ such that $\pi(k) \subseteq Z(R)$.
An \emph{$k$-algebra homomorphism} $f : R \to S$ is simply a ring
homomorphism such that $\pi_S = f \circ \pi_R$.
\end{definition}

Equivalently, an $k$-algebra is both a $k$-module and a ring such
that the structures are compatible.
Observe that this alternative characterization breaks down if $k$ is not
in the center of the overring $R$, since it is not clear whether
$R$ should be a left or right $R$-module.
Thus, the restriction to commutative base rings in the center of the
overring is fairly reasonable.

Over non-commutative rings, there are some subtleties to homomorphisms
and tensor products that can be ignored in the commutative setting.
Here we discuss some of these subtleties.

\begin{definition}
Given a ring $R$, the \emph{opposite ring} $R^{\mathrm{op}}$ has
the same underlying abelian group, but the multiplication is in the
reverse order; in other words, $a \cdot_{\mathrm{op}} b := ba$.
\end{definition}

Note that commutative rings are canonically isomorphic to their
opposite rings by the identity map.
Since the base ring $k$ of a $k$-algebra is always commutative,
the opposite ring of a (possibly non-commutative)
$k$-algebra is still a $k$-algebra.

The transpose map $A \mapsto A^T$ gives a canonical isomorphism
of the matrix ring $\operatorname{M}_n(k)$ with its opposite
$\operatorname{M}_n(k)^{\mathrm{op}}$.
The transpose map $g \to g^{-1}$ gives a canonical isomorphism
of the group ring $\mathbb{Z}G$ with its opposite
$\mathbb{Z}G^{\mathrm{op}}$.

An important example for our purposes shows that we cannot expect
canonical isomorphisms in general:

\begin{example}
Let $V$ be a finite-dimensional vector space with dual $V^\vee$
and consider the ring $\operatorname{End}(V)$.
We have a canonical isomorphism
\[
\psi : \operatorname{End}(V)^{\mathrm{op}} \to \operatorname{End}(V^\vee)
\]
via $\psi(f)(g):=g \circ f$ for $f \in
\operatorname{End}(V)^{\mathrm{op}}$ and $g \in V^\vee$.
We obtain a (non-canonical) isomorphism between
$\operatorname{End}(V)^{\mathrm{op}}$ and $\operatorname{End}(V)$
by way of a choice of isomorphism $V \cong V^\vee$.
\end{example}

Despite these example, rings are not necessarily isomorphic to their opposite.
We will see later that this does not even hold for \emph{division
rings}, which are important for representation theory.
Here is a small, somewhat contrived, example for the impatient: 

\begin{exercise}
Consider the subring $R \subseteq \operatorname{M}_2(\mathbb{Q})$
given by
\[
R = \left\{ \begin{pmatrix} a & b\\ 0 & c \end{pmatrix}\ \middle|\
a \in \mathbb{Z}, b \in \mathbb{Q}, c \in \mathbb{Q} \right\}.
\]
Prove that $R$ is not isomorphic to $R^{\mathrm{op}}$.
\end{exercise}

Observe that every left $R$-module is a right $R^{\mathrm{op}}$-module
via the scalar multiplication $m \cdot_{\mathrm{op}} r := rm$.
Similarly, every right $R$-module is a left $R^{\mathrm{op}}$-module.
In particular, as we have already observer,
the distinction between left and right
$R$-modules is inconsequential precisely when $R$ is commutative.

\begin{definition}
Let $R$ and $S$ be rings.  An \emph{$(R,S)$-bimodule} $M$ is a left
$R$-module that is also a right $S$-module such that $(rm)s=r(ms)$ for
all $r$.
\end{definition}

\begin{exercise}
Show that being an $(R,S)$-bimodule is equivalent to being a
left $R \times S^{\mathrm{op}}$-module, and also to being
a right $R^{\mathrm{op}} \times S$-module.
\end{exercise}

Bimodules are a quite natural object:

\begin{example}
The ring $R$ is in particular an $(R,R)$-bimodule.
\end{example}

\begin{example}
Let $V$ and $W$ be $k$-modules.  The set of homomorphisms
$\operatorname{Hom}_k(V,W)$ is a
$(\operatorname{End}_k(W),\operatorname{End}_k(V))$-bimodule
via $fgh := f \circ g \circ h$.
\end{example}

Every left $R$-module has a canonical structure of a
$(R,\mathbb{Z})$-bimodule; indeed even a $(R,Z(R))$-bimodule where
$Z(R)$ is the center of $R$.
Similarly, every right $R$-module has a canonical structure of
a $(Z(R),R)$-bimodule.
As a consequence, if $R$ is a $k$-algebra, then every left or right
$R$-module is canonically both a left and right $k$-module.

\begin{definition}
If $M$ and $N$ are left $R$-modules, then
\[
\operatorname{Hom}_R(M,N) = \operatorname{Hom}_{R-\operatorname{mod}}(M,N)
\]
is the set of left $R$-module homomorphisms $f : M \to N$.
If $M$ and $N$ are right $R$-modules, then
\[
\operatorname{Hom}_{\operatorname{mod}-R}(M,N)
\]
is the set of right $R$-module homomorphisms $f : M \to N$.
\end{definition}

Suppose that $S$ and $T$ are rings,
$M$ is an $(R,S)$-bimodule, and $N$ is an $(R,T)$-bimodule.
Then $\operatorname{Hom}_R(M,N)$ has a $(S,T)$-bimodule structure
via $(s\phi t)(m):=\phi(ms)t$ for $s \in S, m \in M, t\in T$.
Similarly, if $M$ is an $(S,R)$-bimodule and $N$ is a $(T,R)$-bimodule,
then $\operatorname{Hom}_{\operatorname{mod}-R}(M,N)$
has a $(T,S)$-bimodule structure.

Thus, when $R$ is commutative, $\operatorname{Hom}_R(M,N)$
has a canonical $R$-module structure due to the canonical
$(R,R)$-bimodule structure on $M$ (and $N$).
More generally, $\operatorname{Hom}_R(M,N)$ has only a $Z(R)$-module
structure.
If $R$ is a $k$-algebra, then $\operatorname{Hom}_R(M,N)$
is canonically a $k$-module.
At the very least $\operatorname{Hom}_R(M,N)$ is always an
abelian group due to the canonical $\mathbb{Z}$-module structure.

\begin{example}
Suppose $V$ and $W$ are representations of a finite group $G$
over a field $k$.
Then
\[
\operatorname{Hom}_{kG}(V,W) = \operatorname{Hom}^G_k(V,W)
\]
is the $k$-vector space of $G$-equivariant linear transformations
$f : V \to W$.
\end{example}

\begin{definition}
Suppose $M$ is right $R$-module, $N$ is a left $R$-module,
and $A$ is an abelian group.
A group homomorphism $f: M \times N \to A$ is \emph{$R$-balanced}
if $f(mr,n)=f(m,rn)$ for all $r \in R$, $m \in M$, and $n \in N$.
A \emph{tensor product} $M \otimes_R N$ is an abelian group
together with a $R$-balanced group homomorphism
$\pi : M \times N \to M \otimes_R N$ such that
for any $R$-balanced group homomorphism
$\phi : M \times N \to A$ there exists a unique
group homomorphism $\psi :  M \otimes_R N \to A$
such that $\phi = \psi \circ \pi$.
\end{definition}

Once again, we have:

\begin{proposition}
Tensor products exist and are unique up to unique isomorphism.
\end{proposition}

Just like with hom-sets, the tensor product carries additional
structures when the ingredients are bimodules.
If $M$ is an $(S,R)$-bimodule and $N$ is an $(R,U)$-bimodule,
then $M \otimes_R N$ is an $(S,U)$-bimodule via the multiplication
$s(m\otimes n)r := (sm) \otimes (nr)$ extended by linearity.

Many constructions use the various canonical bimodule structures
implicitly.
For example, if $R$ is commutative, and $M$ and $N$ are both
\emph{left} $R$-modules, then we can use the canonical
\emph{right} $R$-module structure on $M$
to make sense of $M \otimes_R N$.
Then $M \otimes_R N$ has an $R$-module structure using either the left
$R$-module structure on $M$ or the right $R$-module structure on $N$
(which agree).
For annother example, if $R$ is a $k$-algebra, then $M \otimes_R N$
has a canonical $k$-module structure.

We are now in a position to state an important result:

\begin{theorem}[Tensor-Hom Adjunction]
Let $R$, $S$, $U$, $V$ be rings.
Let $M$ be an $(R,S)$-bimodule, $N$ be an $(S,U)$-bimodule,
and $P$ be an $(R,V)$-bimodule.
Then there is a natural isomorphism
\[
\operatorname{Hom}_R(M \otimes_S N,P)
\cong \operatorname{Hom}_S(N,\operatorname{Hom}_R(M,P))
\]
 of $(U,V)$-bimodules.
\end{theorem}

\begin{exercise}
The hom-sets in the statement of the Tensor-Hom Adjunction
use the \emph{left} module structures on $M,N,P$ etc.
Determine the analogous statement where the hom-sets
refer to the \emph{right} module structures.
\end{exercise}

Rarely are all bimodule structures needed at once!
The most important special case is when $M$ is an $(R,S)$-bimodule,
$N$ is a left $S$-module and $P$ is left $R$-module;
the resulting isomorphism is then just of abelian groups.

One of the main applications of the Tensor-Hom Adjunction is
understanding restriction and extension of scalars.

\begin{definition}
Let $f : R \to S$ be a ring homomorphism.
If $N$ is a left $S$-module, then the \emph{restriction of scalars
of $N$} is the left $R$-module structure on $N$ given by
$r \cdot_R n := f(r)n$ for $r \in R$ and $n \in N$.
We often denote the restriction of scalars by ${}_RN$
or${}_fN$ to emphasize the distinction with the original $N$
viewed as a left $S$-module.
\end{definition} 

When $f$ is the inclusion of a subring, one can think of the restriction
of scalars as ``forgetting'' some of the structure of $N$.
For example, a complex vector space $\mathbb{C}^n$ becomes the real
vector space $\mathbb{R}^{2n}$ under restriction by the inclusion
$\mathbb{R} \to \mathbb{C}$.

\begin{example}
If $H$ is a subgroup of a group $G$, the we have an inclusion
$kH \to kG$ of group algebras.
Let $V$ be a representation of a finite group $G$ over a field $k$.
Then $V$ has the structure of a left $kG$-module.
The restriction of scalars ${}_{kH}V$ is the left $kH$-module
corresponding to $\operatorname{Res}_H^G V$. 
\end{example}

Somewhat trickier is going in the other direction.

\begin{definition}
Let $f : R \to S$ be a ring homomorphism and $M$ be an $R$-module.
Let $S_R$ be the corresponding $(S,R)$-bimodule structure on $S$
and let ${}_RS$ be the corresponding $(R,S)$-bimodule structure on $S$.
The \emph{extension of scalars of $M$}
is the left $S$-module $S_R \otimes_R M$.
The \emph{coextension of scalars of $M$}
is the left $S$-module $\operatorname{Hom}_R({}_RS,M)$.
\end{definition}

Note that the notation $S_R$ and ${}_RS$ is usually considered
unnecessarily pedantic.
We typically write simply $S \otimes_R M$ for extension of scalars
and $\operatorname{Hom}_R(S,M)$ for coextension of scalars.
This is usually unambiguous, although perhaps confusing for beginners.

The Tensor-Hom adjunction shows that extension of scalars
and restriction of scalars are adjoint.
Indeed, viewing $S$ as an $(S,R)$-bimodule,
we have $\operatorname{Hom}_S(S,M) \cong {}_RM$.
Thus, the adjunction becomes
\[
\operatorname{Hom}_R(S \otimes_R N,M)
\cong \operatorname{Hom}_R(N,{}_RM)
\]
for a left $R$-module $N$ and a left $S$-module $M$.

Once again, these have immediate connections to representation theory:

\begin{example}
If $H$ is a subgroup of a group $G$, the we have an inclusion
$kH \to kG$ of group algebras.
Let $W$ be a representation of a finite group $H$ over a field $k$.
Then $W$ has the structure of a left $kH$-module.
The extension of scalars $kG \otimes_{kH} W$
is the left $kG$-module corresponding to $\operatorname{Ind}_H^G W$.
\end{example} 

To see why extension of scalars corresponds to induction, we just need
to recall that we essentially \emph{defined} induction to be the linear
representation that satisfied (a version of) Frobenius reciprocity.
The Tensor-Hom adjunction gives us an isomorphism of $k$-vector spaces
\[
\operatorname{Hom}_{kG}(kG \otimes_{kH} W,V)
\cong \operatorname{Hom}_{kH}(W,{}_{kH}V))
\]
which is exactly the isomorphism
\[
\operatorname{Hom}_k^G(\operatorname{Ind}_H^G W,V)
\cong \operatorname{Hom}_k^H(W,\operatorname{Res}_H^G V)
\]
from Frobenius Reciprocity!

We leave it as an exercise to check that coextension of scalars
corresponds to coinduction.  Indeed, the distinction is not important
(at least in our setting) in view of the following:

\begin{exercise}
Suppose $H$ is subgroup of finite group $G$ and $W$ is a $kH$-module.
Prove that
\[
kG \otimes_{kH} W \cong \operatorname{Hom}_{kH}(kG,W)
\]
as left $kG$-modules.
\end{exercise}

\section{Wedderburn Decomposition}

Throughout this section, $k$ is a field.
The main goal of this section is to prove the following:

\begin{theorem}[Wedderburn's Theorem] \label{thm:Wedderburn}
A finite dimensional $k$-algebra $A$ is \emph{semisimple}
if and only if
\[
A \cong \bigoplus_{i=1}^r \operatorname{M}_{n_i}(D_i)
\]
where $D_1, \ldots, D_r$ are finite-dimensional $k$-algebras
and $n_1,\ldots, n_r$ are positive integers.
\end{theorem}

\begin{definition}
Let $A$ be a $k$-algebra and $M$ be a left $A$-module.
A \emph{composition series} or \emph{simple finite filtration}
is a descending sequence of submodules
\[
M=M_0 \supsetneq M_1 \supsetneq \cdots \supsetneq M_r = 0
\]
such that each quotient $M_i/M_{i+1}$ is simple.
The integer $r$ is the \emph{length} of the filtration.
The module $M$ is of \emph{finite length} if it possesses a simple finite
filtration or $M=0$.
\end{definition}

\begin{theorem}[Jordan-H\"older]
Any two composition series of a left module $M$ are \emph{equivalent}:
the isomorphism classes of the quotients $M_i/M_{i+1}$
are unique up to reordering.
\end{theorem}

In view of the Jordan-H\"older theorem, the following notion is
well-defined:

\begin{definition}
The \emph{length} of a module $M$ of finite length is the length of any
composition series (or $0$ if $M=0$).
\end{definition}

Recall that a ring is Artinian (resp. Noetherian) if it satisfies the
descending chain condition (resp. ascending chain condition) on ideals.
Similarly, a module is Artinian (resp. Noetherian) if it satisfies the
descending chain condition (resp. ascending chain condition) on
submodules.  Vector spaces over a field are in particular, both Artinian
and Noetherian so we immediately have the following.

\begin{proposition}
Suppose $A$ is a finite-dimensional $k$-algebra,
and $M$ is a finitely generated left $A$-module.
Then $A$ and $M$ are both Noetherian and Artinian.
In particular, we see that $M$ has finite length.
\end{proposition}

If $M$ is a semisimple left module, then there is a decomposition
\begin{equation} \label{eq:isoDecomp}
M = M_1 \oplus \cdots \oplus M_r,
\end{equation}
where each $M_i$ is a submodule isomorphic to $S_i^{\oplus a_i}$,
$S_i$ is simple, $a_i$ is a positive integer
and $S_i \not\cong S_j$ when $i \ne j$.
Note that the each submodule $M_i$ is canonical, but the
isomorphism $M_i \cong S_i^{\oplus a_i}$ is not.
The submodules $M_1, \ldots, M_r$ are called the \emph{isotypic
components} of $M$, and \eqref{eq:isoDecomp} is called the \emph{isotypic
decomposition} of $M$.

\begin{lemma} \label{lem:ssModule}
Let $A$ be a finite-dimensional $k$-algebra and $M$ be a finitely
generated left $A$-module.
The following are equivalent:
\begin{enumerate}
\item[(a)] every submodule of $M$ is a direct summand,
\item[(b)] $M$ is semisimple, and
\item[(c)] $M$ is sum of simple submodules (not a priori direct).
\end{enumerate}
\end{lemma}

\begin{proof}
If $N_1 \subset N_2 \subset M$ is a chain of submodules and $N_1$ is a
direct summand of $M$ then $N_1$ is a direct summand of $N_2$.
Thus (a) $\implies$ (b) follows by induction on the dimension of $M$,
with simple modules as the base case.  The implication (b) $\implies$
(c) is immediate.  It remains to show (c) $\implies$ (a).

Let $N$ be a submodule of $M$.
Let $N'$ be the maximal submodule of $M$ such that $N \cap N' = 0$
(which exists since $M$ is finite-dimensional).
If $N + N' = M$ then we are done, so suppose $N + N' \subsetneq M$.
Then there is a simple submodule $S$ of $M$ not contained in $N+N'$;
in fact, $S \cap (N+N')=0$ since it is simple.

In particular, $N' \subsetneq N'+S$.
Consider $m \in (N'+S) \cap N$.  We have $m=n+s$ where and $n \in N'$
and $s \in S$.  The element $s=m-n$ is in both $S$ and $N+N'$, so is
trivial.  Thus $m \in N' \cap N$ is itself trivial.
We conclude that $(N'+S) \cap N$ is trivial.
The module $N'+S$ is a counterexample to the assumption of maximality of
$N'$.
\end{proof}

\begin{lemma} \label{lem:subquot}
Submodules and quotient modules of semisimple modules are semisimple.
\end{lemma}

\begin{proof}
Submodules are semisimple by an argument similar to
(1) $\implies$ (2) from Lemma~\ref{lem:ssModule}.
Quotient modules are semisimple by Schur's lemma and
condition (3) from Lemma~\ref{lem:ssModule}.
\end{proof}

Recall that an algebra $A$ is semisimple if and only if it is a direct
product of simple rings.  We will see that this is equivalent to $A$
being semisimple as a left module over itself.  In the interim, we prove
a third equivalent restatement:

\begin{lemma}
Let $A$ be a finite-dimensional $k$-algebra.
Every finitely generated left $A$-module is semisimple if and only if
${}_AA$ is semisimple as a left $A$-module over itself.
\end{lemma}

\begin{proof}
Suppose ${}_AA$ is semisimple as a left $A$-module.
Any $A$-module $M$ can be written as a quotient of the free $A$-module
$A^n$ for some positive integer $n$.
Thus any submodule $M$ is semisimple by Lemma~\ref{lem:subquot}.
The converse follows a fortiori.
\end{proof}

As a consequence of the previous lemma, along with
Jordan-H\"older, we conclude that a semisimple algebra has only finitely
many isomorphism classes of simple modules!

\begin{lemma}
Let $A$ be a finite-dimensional $k$-algebra.
If $A$ is a simple algebra, then ${}_AA$ is semisimple.
\end{lemma}

\begin{proof}
Let $M$ be the sum of all simple submodules in ${}_AA$.
If $S$ is a simple submodule in ${}_AA$ and $a \in A$, then the left
ideal $Sa$ is either $0$ or a simple submodule of ${}_AA$.
Thus $Ma \subseteq M$ for any $a \in A$.
Thus $M$ is a right ideal of $A$ so, in fact, a two-sided ideal.
Since $M \ne 0$ and $A$ is simple as a ring, $M=A$.
So ${}_AA$ is a sum of simple submodules, and thus is semisimple.
\end{proof}

\begin{corollary} \label{cor:semisimple_def}
Let $A$ be a finite-dimensional $k$-algebra.
If $A$ is a semisimple algebra, then ${}_AA$ is semisimple.
\end{corollary}

\begin{proof}
Let $A = A_1 \oplus \cdots \oplus A_n$ where each $A_i$ is a simple
$k$-algebra. Observe that the left $A$-module structure on ${}_AA_i$
factors through the $A_i$-module structure.
Each $A_i$ is, in particular, a submodule of ${}_AA$.
Since they are semisimple, so is their sum.
\end{proof}

(The converse of the corollary will follow as a consequence of
Wedderburn's theorem.)

A proof of the following is left as an exercise:

\begin{lemma} \label{lem:fancyMatrix}
Let $A$ be a ring.
Let $M= M_1 \oplus \cdots \oplus M_m$ and
$N=N_1 \oplus \cdots \oplus N_n$ be direct sums of $A$-modules.
Then
\[ \operatorname{Hom}_A\left(\bigoplus_iM_i,\ \bigoplus_jN_i\right)
\cong \bigoplus_{i,j}\operatorname{Hom}_A(M_i, N_j) \ . \]
\end{lemma}

\begin{lemma}
Let $A$ be a finite-dimensional $k$-algebra and
let $M$ be a semisimple left $A$-module.
Suppose
\[
M = M_1 \oplus \cdots \oplus M_r
\]
is the isotypic decomposition where $M_i=S_i^{\oplus n_i}$
for some simple $A$-module $S_i$.
There is a canonical isomorphism of $k$-algebras
\[
\operatorname{End}_A(M) \cong \operatorname{M}_{n_1}(\operatorname{End}_A(S_1))
\oplus \cdots \oplus
\operatorname{M}_{n_r}(\operatorname{End}_A(S_r)) .
\]
\end{lemma}

\begin{proof}
By Lemma~\ref{lem:fancyMatrix},
we have $\operatorname{End}_A(N) = \operatorname{Hom}_A(N,N)$ and
$\operatorname{End}_A(M^{\oplus n}) \cong M_n(\operatorname{End}_A(N))$
for any left $A$-module $N$ and any positive integer $n$.
Thus, it suffices
to show that $\operatorname{Hom}_A(M_i,M_j)=0$ whenever $i \ne j$.
Note that $M_i=S_i^{\oplus n_i}$ and $M_j=S_j^{\oplus n_j}$
for integers $n_i, n_j$ and
non-isomorphic simple modules $S_i$ and $S_j$.
Again using the previous lemma, $\operatorname{Hom}_A(M_i,M_j)$ is a direct sum of
modules of the form $\operatorname{Hom}_A(S_i,S_j)$.  These are $0$ by Schur's lemma.
\end{proof}

\begin{lemma}
Let $A$ be a $k$-algebra.
There is a canonical isomorphism $A^{\mathrm{op}} \cong \operatorname{End}_A({}_AA)$ of
$k$-algebras.
\end{lemma}

\begin{proof}
Given an element $a \in A^{\mathrm{op}}$,
there is a unique endomorphism $\phi_a \in A^{\mathrm{op}}$
such that $\phi_a(1)=a$.
This gives a correspondence that commutes with addition and scalar
multiplication.
Thus there is an isomorphism between the underlying vector spaces.
The computation
\[ (\phi_a \circ \phi_b)(1)=\phi_a(b)=b\phi_a(1)=ba \]
shows that the multiplications agree.
Thus the algebras are isomorphic.
\end{proof}

One more observation before the main theorem:

\begin{lemma}
For a $k$-algebra $A$, there is a
$k$-algebra isomorphism $\operatorname{M}_n(A)^{\mathrm{op}}
\cong \operatorname{M}_n(A^{\mathrm{op}})$.
\end{lemma}

\begin{proof}
Take a matrix to its transpose.  This is clearly a vector space
isomorphism.  A short calculation shows that the multiplications are
also compatible. 
\end{proof}

We now prove Wedderburn's theorem.

\begin{lemma}
Let $A$ be a finitely generated $k$-algebra.
If ${}_AA$ is semisimple as a left $A$-module,
then $A$ is is isomorphic to a direct
sum of matrix algebras over division algebras.
\end{lemma}

\begin{proof}[Proof of Theorem~\ref{thm:Wedderburn}]
Suppose $A$ is semisimple as a $k$-algebra.
Then ${}_AA$ is semisimple as a left $A$-module
by Corollary~\ref{cor:semisimple_def}.

Since ${}_AA$ is semisimple, it has an isotypic decomposition
\[
{}_AA = S_1^{\oplus n_1} \oplus \cdots \oplus S_r^{\oplus n_r}
\]
where $S_i$ are pairwise non-isomorphic simple modules.
Let $D_i=\operatorname{End}_A(S_i)$,
which are division algebras by Schur's Lemma.

Thus
\begin{align*}
A^{\operatorname{op}} &\cong \operatorname{End}_A(A)\\
&\cong \operatorname{End}_A(S_1^{\oplus n_1}) \oplus \cdots
\oplus \operatorname{End}_A(S_r^{\oplus n_r})\\
&\cong M_{n_1}(D_1) \oplus \cdots \oplus M_{n_r}(D_r) \ ,
\intertext{and applying the last lemma:}
A &\cong M_{n_1}(D_1^{\operatorname{op}}) \oplus \cdots \oplus
M_{n_r}(D_r^{\operatorname{op}}) \ .
\end{align*}

We leave the converse as an exercise.
\end{proof}

Note that a corollary of the above (one direction of which was used in
the proof) is the following characterization of semisimplicity of an
algebra.

\begin{corollary}
Let $A$ be a finite-dimensional $k$-algebra.
The following are equivalent:
\begin{itemize}
\item $A$ is semisimple as a ring (a direct sum of simple rings).
\item $A$ is semisimple as a left $A$-module (a direct sum of simple
modules).
\item Every finitely generated left $A$-module is semisimple.
\end{itemize}
\end{corollary}

\section{Central Simple Algebras}

As in the previous section, $k$ is a field.
Some of the results here are elaborated in texts on central simple
algebras and the Brauer group (for example, \cite{GS} or \cite{Saltman}). 

Since a semisimple algebra is a direct sum of simple algebras,
we are now interested in studying simple algebras.
The most important invariant of a simple algebra is its center,
which is always a field since otherwise there is a non-trivial two-sided
ideal.  This motivates the following definition.

\begin{definition}
A \emph{central simple $k$-algebra} is a finite-dimensional $k$-algebra
$A$ with center $Z(A)=k$.
A \emph{central division $k$-algebra} is a finite-dimensional $k$-algebra
$A$ with center $Z(A)=k$.
\end{definition}

In view of Wedderburn's theorem, we may characterize central simple
algebras completely.  We summarize as follows:

\begin{proposition}
Suppose $A$ is a finite-dimensional $k$-algebra.
\begin{enumerate}
\item If $A$ is semisimple, then it is a product of simple $k$-algebras.
\item If $A$ is simple, then it is a central simple $K$-algebra, where
$K$ is a finite field extension of $k$.
\item If $A$ is a central simple $k$-algebra, then $A \cong
\operatorname{M_n}(D)$ where $D$ is a central division $k$-algebra.
\end{enumerate}
\end{proposition}

The following shows that everything boils down to understanding division
algebras.

\begin{proposition}
If $A$ is a central simple $k$-algebra, then there is a unique central
division $k$-algebra $D$ (up to isomorphism) and positive integer $n$
such that $A \cong \operatorname{M}_n(D)$.
\end{proposition}

\begin{proof}
Suppose $A \cong M_n(D) \cong M_m(E)$ for central division $k$-algebras
$D$ and $E$.
Let $L$ be a minimal left ideal of $A$.
Then $D^{\oplus n} \cong L \cong E^{\oplus m}$ since all the algebras
are simple and have only isomorphism class of simple module.
One may view $D^{\oplus n}$ as the set of column vectors with
entries in $D$.
Observe that an endomorphism $D^{\oplus n} \to D^{\oplus n}$
as left $A$-modules is wholly determined by the image of
the element $(1,0,\ldots,0)$.  One checks that the only
permitted images are of the form $(a,0,\ldots,0)$ for $a \in A$.
Thus $\operatorname{End}_A(D^{\oplus n}) \cong D$.
Thus
\[
D \cong \operatorname{End}_A(D^{\oplus n})
\cong L
\cong \operatorname{End}_A(E^{\oplus n}) \cong E
\]
and $m=n$ follows by dimension reasons.
\end{proof}

\begin{lemma}[Rieffel]
Let $A$ be a simple $k$-algebra, let $L$ be a non-zero left ideal in
$A$, and let $D = \operatorname{End}_A(L)$.
The left multiplication map $\lambda : A \to \operatorname{End}_D(L)$
is a $k$-algebra isomorphism.
\end{lemma}

\begin{proof}
First note that $L$ has a natural left $D$-module structure
via $\varphi \cdot x := \varphi(x)$ for $\varphi \in D$ and $x \in L$.
The left multiplication map is given by taking $a \in A$ to
the map $\lambda_a : L \to L$ defined by $\lambda_a(x)=ax$
for $x \in L$.
Note that every $\lambda_a$ is indeed a left $D$-module homomorphism
of $L$ since $\varphi \cdot ax = \varphi(ax)=a\varphi(x)=a(\varphi \cdot
x)$ for all $a \in A$, $\varphi \in D$, and $x \in X$.
It is evident that $\lambda$ is a $k$-algebra homomorphism.

Since $A$ is simple and $\lambda \ne 0$, the two-sided ideal
$\ker(\lambda)$ must be trivial.  Thus $\lambda$ is injective.
It remains to show surjectivity.

We show $\lambda(L)$ is a left ideal of $\operatorname{End}_D(L)$.
First, observe that for $x \in L$,
consider the right multiplication map $\rho_x : L \to L$
given by $\rho_x(y)=yx$.  We see that $\rho_x$ is an endomorphism
of $A$-modules and, thus, an element of $D$.
Suppose $x, \ell \in L$ and $\varphi \in \operatorname{End}_D(L)$.
We have
\[
(\varphi \circ \lambda(\ell))(x)=\varphi(\ell x)
=\varphi(\rho_x(\ell))=\rho_x(\varphi(\ell))=\varphi(\ell)x .
\]
Since $x$ was arbitrary, we conclude $\varphi \cdot \lambda(\ell)
= \lambda(\varphi\cdot \ell)$.  Thus $L$ is a left ideal.

Now observe that the right ideal $LA$ is a two-sided ideal,
so $LA=A$ since $A$ is simple.
Thus, we have
\[
1 = \sum_i \ell_i a_i
\]
for some $\ell_i \in L$ and $a_i \in A$.
Given an arbitrary $\varphi \in \operatorname{End}_D(L)$,
we write
\[
\varphi = \varphi \cdot 1 = \sum_i \varphi \lambda(\ell_i)\lambda(a_i).
\]
But, $\varphi \lambda(\ell_i) \in \lambda(L)$ since $\lambda(L)$
is a left ideal.  Thus $\varphi$ is in the image of $\lambda$.
\end{proof}

\bibliographystyle{alpha}
\bibliography{rep_theory}

\end{document}


