\documentclass[12pt]{article}

\usepackage{amssymb,amsmath,amsthm}

% theorem styles (I like everything to have the same counter)
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{exercise}[theorem]{Exercise}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% some numbering settings
\numberwithin{equation}{section}

\usepackage{fancyhdr}
\usepackage{lastpage}
\setlength{\headheight}{15.2pt}
\renewcommand{\footrulewidth}{0.4pt}% default is 0pt
\setlength{\footskip}{30pt}
\pagestyle{fancy}

\makeatletter
\let\ps@plain\ps@fancy 
\makeatother

\lhead{MATH 742}
\chead{Multilinear Algebra}
\rhead{Spring 2023}
\lfoot{Last Revised: \today}
\cfoot{}
\rfoot{\thepage\ of \pageref{LastPage} }

\begin{document}

%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%
\title{Multilinear Algebra}
\author{Alexander Duncan}

\maketitle

Here we review and/or introduce standard facts from (multi)linear algebra.
Much of this should have been seen in undergraduate linear algebra or the
standard algebra qualifying exam sequence, but some ideas (such as
tensor products) may not have been.

Two standard references for graduate level algebra are
\cite{DF} and \cite{Lang}.  We focus on the case where the base ring is
a field, since this is our main application.
Many of these ideas can be generalized or extended to arbitrary rings,
but we do not do so here.

\section{Endomorphisms}

Let $k$ be a field.

\begin{definition}
A \emph{$k$-algebra} $A$ is both a $k$-vector space and a unital ring
such that $r(ab)=(ra)b=a(rb)$ for all $r \in k$ and $a,b \in A$.
A \emph{morphism of $k$-algebras} $f \colon A \to B$ is both a
$k$-linear transformation and a ring homomorphism.
\end{definition}

An alternative (equivalent) definition of $k$-algebra is as a
unital ring $A$ along with ring homomorphism $\pi : k \to A$,
called the \emph{structure morphism}, whose image is contained in
the center of $A$.  Using this definition,
a morphism of $k$-algebras is a ring homomorphism $f\colon A \to B$ such
that $\pi_B = f \circ \pi_A$, where $\pi_A$ and $\pi_B$ are the
respective structure morphisms of $A$ and $B$.

\begin{exercise}
Prove the two definitions of $k$-algebra (resp. $k$-algebra morphism) are
equivalent.
\end{exercise}

\begin{definition}
Let $\operatorname{M}_{n,m}(k)$ be the set of
$n \times m$ matrices with coefficients in $k$ and
let $\operatorname{M}_{n}(k) = \operatorname{M}_{n,n}(k)$. 
\end{definition}

The set of matrices $\operatorname{M}_{n,m}(k)$ is a $k$-vector space
and the set of square matrices $\operatorname{M}_n(k)$ is a $k$-algebra
under matrix multiplication.

\begin{definition}
For $k$-vector spaces $V,W$, let
$\operatorname{Hom}_k(V,W)$ be set of $k$-linear
transformations from $V$ to $W$.
We write $\operatorname{End}_k(V) = \operatorname{Hom}_k(V,V)$
for the set of \emph{endomorphisms} of $V$.
We write $V^\vee = \operatorname{Hom}_k(V,k)$
for the \emph{dual space} of $V$.
\end{definition}

The set $\operatorname{Hom}_k(V,W)$ is a $k$-vector space and the set
$\operatorname{End}_k(V)$ is moreover a $k$-algebra under composition.

Given a matrix $A \in \operatorname{M}_{n,m}(k)$, we obtain
two canonical maps.
Namely, we have \emph{left multiplication}
$L_A \in \operatorname{Hom}(k^m,k^n)$ via $L_A(v)=Av$
viewing $v$ as a column vector
and \emph{right multiplication}
$R_A \in \operatorname{Hom}(k^n,k^m)$ via $R_A(v)=vA$
viewing $v$ as a row vector.

\begin{theorem}
Suppose $\psi: k^n \to V$ and $\phi : k^m \to W$ are isomorphisms of
vector spaces.
Then there is an vector space isomorphism
$\operatorname{M}_{n,m}(k) \to \operatorname{Hom}_k(V,W)$ given by
$A \mapsto \phi \circ L_A \circ \psi^{-1}$.
In the case where $W=V$ and $\phi=\psi$, this becomes a $k$-algebra
isomorphism $\operatorname{M}_n(k) \to \operatorname{End}_k(V)$.
\end{theorem}

\begin{proof}
Exercise.
\end{proof}

\subsection{Dual Spaces}

If $V$ is a vector space with finite basis
$\beta_1, \ldots, \beta_n$, then there is a unique \emph{dual basis}
$\beta^\vee_1, \ldots, \beta^\vee_n$
for $V^\vee$ such that $\beta_i^\vee(\beta_j)=\delta_{ij}$,
where $\delta_{ij}$ is the Kronecker delta.

In particular, for a finite-dimensional vector space $V$,
there is a \emph{non-canonical} isomorphism $V \cong V^\vee$,
which depends on a choice of basis.
In contrast, there is a \emph{canonical} isomorphism
$V \cong (V^\vee)^\vee$ by taking a vector $v \in V$ to the
``evaluation'' functional $\operatorname{ev}_v : f \mapsto f(v)$. 

\subsection{Characteristic Polynomial}

\begin{definition}
Given a square matrix $A \in \operatorname{M}_n(k)$, we define the
\emph{characteristic polynomial}:
\[
\chi_A(t) = \det(tI-A)
\]
which is an element of $k[t]$.
We use the sign convention ensuring that the
characteristic polynomial is always monic.

The \emph{minimal polynomial} $m_A(t) \in k[t]$ is the monic generator
of the principal ideal $\ker(\psi_A)$ where
$\psi_A : k[t] \to \operatorname{M}_n(k)$ is the $k$-algebra
homomorphism determined by $\psi_A(t)=A$.
\end{definition}

\begin{theorem}[Cayley-Hamilton]
$m_A(t)$ divides $\chi_A(t)$
\end{theorem}

\begin{definition}
The multiset of \emph{eigenvalues} of $A \in \operatorname{M}_n(k)$ is
the multiset of roots of $\chi_A(t)$ over the algebraic closure
$\overline{k}$ of $k$.
\end{definition}

If $\lambda_1, \ldots, \lambda_n$ are the eigenvalues of $\chi_A(t)$,
counted with multiplicity,
then we have
\[
\chi_A(t) = t^n - e_1 t^{n-1} + e_2 t^{n-2} - e_3 t^{n-3} + \cdots \pm e_n
\]
where $e_1,\ldots, e_n$ are the \emph{elementary symmetric functions}
in $\lambda_1,\ldots, \lambda_n$.

As a formula, we have that the elementary symmetric function $e_i$
is given by
\[
e_i = \sum_{X \in S(n,k)} \left(\prod_{i \in X} \lambda_i\right)
\]
where $S(n,k)$ is the set of subsets of $\{1,\ldots,n\}$ of size
$k$.

Important special cases are the trace
\[
\operatorname{tr}(A) = e_1 = \lambda_1 + \ldots + \lambda_n
\]
and the determinant
\[
\det(A) = e_n = \lambda_1 \lambda_2 \cdots \lambda_{n-1} \lambda_n .
\]

Recall that two matrices $A,B \in \operatorname{M}_n(k)$ are \emph{similar}
if there exists a matrix $P$ such that $A=PBP^{-1}$.
Given an endomorphism $f$ of a $n$-dimensional vector space $V$,
one obtains different representing matrices for $f$ depending on the
choice of basis.  However, the set of matrices representing $f$ is
a similarity class in $\operatorname{M}_n(k)$.

The minimal polynomial, characteristic polynomial, eigenvalues,
trace, and determinant are all invariant under similarity.
Thus, one can define the minimal polynomial,
characteristic polynomial, eigenvalues, trace, and determinant of an
endomorphism in a canonical way that does not depend on the choice of
basis.

\subsection{Jordan Canonical Form}

The \emph{Jordan block of size $n$ with eigenvalue $\lambda$} is the
$n$-dimensional matrix
\[
J_n(\lambda) = \begin{pmatrix}
\lambda & 1 & 0 & \cdots & 0 & 0\\
0 & \lambda & 1 & \cdots & 0 & 0\\
0 & 0& \lambda & \cdots & 0 & 0\\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
0 & 0 & 0 & \cdots & \lambda & 1\\
0 & 0 & 0 & \cdots & 0 & \lambda\\
 \end{pmatrix} .
\]

\begin{definition}
A matrix $A$ is in \emph{Jordan Canonical Form} if $A$ is of block
diagonal form with the blocks being Jordan blocks:
\[
A = \begin{pmatrix}
J_{n_1}(\lambda_1) & \cdots & 0\\
\vdots & \ddots & \vdots\\
0 & \cdots & J_{n_r}(\lambda_r)\\
\end{pmatrix} 
\]
where $n_1,\ldots,n_r$ and $\lambda_1,\ldots,\lambda_r$
are not necessarily distinct.
\end{definition}

Note that \emph{diagonal matrices} are precisely those in Jordan
canonical form where $n_1=\ldots=n_r=1$.
A matrix is \emph{diagonalizable} if it is similar to a diagonal matrix.

\begin{theorem}
Suppose $A$ is a square matrix whose eigenvalues are defined over $k$.
Then $A$ is similar to matrix in Jordan Canonical Form,
which is unique up to reordering the Jordan blocks.
\end{theorem}

Note that the condition in the theorem above always holds when the
field $k$ is algebraically closed.

\begin{exercise}
\[J_n(\lambda)^m =
\begin{pmatrix}
\lambda^m & \binom{m}{1}\lambda^{m-1} & \binom{m}{2}\lambda^{m-2}
& \cdots & \binom{m}{n-2}\lambda^{m-n-2} & \binom{m}{n-1}\lambda^{m-n-1}\\
0 & \lambda^m & \binom{m}{1}\lambda^{m-1}
& \cdots & \binom{m}{n-3}\lambda^{m-n-3} & \binom{m}{n-2}\lambda^{m-n-2}\\
0 & 0 & \lambda^m & \cdots &
\binom{m}{n-4}\lambda^{m-n-4} & \binom{m}{n-3}\lambda^{m-n-3}\\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
0 & 0 & 0 & \cdots & \lambda^m & \binom{m}{1}\lambda^{m-1}\\
0 & 0 & 0 & \cdots & 0 & \lambda^m\\
 \end{pmatrix} .\]
\end{exercise}

\begin{corollary}
Suppose $k$ is an algebraically closed field
and $m$ is an integer coprime to the characteristic.
If $A \in \operatorname{M}_n(k)$ has order $m$,
then $A$ is diagonalizable and its eigenvalues are $m$th roots of unity.
\end{corollary}

\begin{proof}
The order of a matrix is a similarity invariant since
$A=PBP^{-1}$ implies $A^m=PB^mP^{-1}$.
Thus, we may assume that $A$ is in Jordan Canonical Form.
Since we know $A^m=I$, from the exercise we conclude that
for every eigenvalue $\lambda_i$ of $A$, we have
$\lambda_i^m = 1$
and moreover $m\lambda_i^{m-1}=0$ if the corresponding Jordan block is
not $1\times 1$.
The first condition forces all the eigenvalues to be $m$th roots of
unity, while the second forces all the blocks to be $1 \times 1$
(since $m\ne 0$).
\end{proof}

The condition that the integer $m$ is coprime to the characteristic is
necessary.  If $k$ has characteristic $p$, then the matrix
\[
\begin{pmatrix} 1 & 1 \\ 0 & 1 \end{pmatrix} 
\]
has order $p$, but is not diagonalizable. 

\subsection{Rational Canonical Form}

TODO

\section{Bilinear Maps}

\begin{definition}
Let $V$ and $W$ be $k$-vector spaces.
A \emph{bilinear map} $b: V \times W \to k$,
is a function such that
\begin{itemize}
\item $b(\lambda v_1+v_2,w) = \lambda b(v_1,w) + b(v_2,w)$, and
\item $b(v,\lambda w_1+w_2) = \lambda b(v,w_1) + b(v,w_2)$
\end{itemize}
for all $v,v_1,v_2 \in V$, $w,w_1,w_2 \in W$ and $\lambda \in k$.
\end{definition}

The set $\operatorname{Bil}_k(V,W)$ of bilinear maps is a $k$-vector
space.
We have a canonical isomorphism
\[
\operatorname{Bil}_k(V,W) \cong
\operatorname{Hom}_k(V,W^\vee)
\]
where $b \in \operatorname{Bil}_k(V,W)$ is taken to $b_1 : V \to W^\vee$
by defining $b_1(v) = b(v,-)$ for each $v \in V$.
Similarly, we have a canonical isomorphism
\[
\operatorname{Bil}_k(V,W) \cong
\operatorname{Hom}_k(W,V^\vee)
\]
where $b \in \operatorname{Bil}_k(V,W)$ is taken to $b_2 : W \to V^\vee$
by defining $b_2(w) = b(-,w)$ for each $w \in W$.

Suppose $V$ has basis $v_1,\ldots,v_n$ and $W$ has basis $w_1,\ldots, w_m$,
then we can form the \emph{matrix} $B$ of the bilinear map $b$ by defining
$B_{ij} = b(v_i,w_i)$.

\begin{exercise}
For a choice of basis for $V$ and $W$, then the following are equivalent
\begin{itemize}
\item $b_1 : W \to V^\vee$ is an isomorphism.
\item $b_2 : V \to W^\vee$ is an isomorphism.
\item $B$ is an invertible matrix.
\end{itemize}
If any of these conditions hold, then we say the bilinear map
is \emph{non-degenerate}.
\end{exercise}

\subsection{Bilinear Forms}

A \emph{bilinear form} on $V$ is a bilinear map $b : V \times V \to k$.
In other words, both components of the product in the domain are the
same vector space.
In this case, the \emph{matrix} $B$ of a bilinear form $b$ is usually formed
with respect to the same basis for both components of the product.

\begin{definition}
A bilinear form $b: V \times V \to k$ is
\begin{itemize}
\item \emph{symmetric} if $b(v,w)=b(w,v)$ for all $v,w \in V$.
\item \emph{skew-symmetric} or \emph{antisymmetric}
if $b(v,w)=-b(w,v)$ for all $v,w \in V$.
\item \emph{alternating} if $b(v,v)=0$ for all $v \in V$.
\end{itemize}
\end{definition}

\begin{exercise}
The set of symmetric (resp. skew-symmetric, resp. alternating) bilinear
forms is a subspace of $\operatorname{Bil}_k(V,V)$.
\end{exercise}

If $b: V \times V \to k$ is a symmetric bilinear form,
then the two canonical maps $b_1 : V \to V^\vee$ and $b_2 : V \to
V^\vee$ are equal.  Thus, a symmetric bilinear form gives rise to
a single choice $V \to V^\vee$ of isomorphism (rather than a pair).

\begin{example}
The \emph{dot product} on $k^n$ is a symmetric bilinear form.
In the case where $k$ does not have characteristic $2$, the form is
non-degenerate and the corresponding isomorphism
$k^n \to (k^n)^\vee$ can be thought of as the
transpose operation, which takes column vectors to row vectors and vice
versa.
\end{example}

\begin{example}
Consider the $2n \times 2n$ block matrix
\[
J = \begin{pmatrix} 0 & I_n \\ -I_n & 0 \end{pmatrix} 
\]
in $k^{2n}$.
The corresponding bilinear form is a non-degenerate alternating bilinear
form.
\end{example}

\begin{example}
If $k$ has characteristic $2$, then the ``dot product''
on $k^n$ is a \emph{degenerate} symmetric bilinear form.
It is also an example of a skew-symmetric bilinear form that is not
alternating.
\end{example}

\begin{proposition}
Every alternating form is skew-symmetric.
\end{proposition}

\begin{proof}
If $b$ is alternating, then
\begin{align*}
0 &= b(v+w,v+w)\\
&= b(v,v) + b(v,w) + b(w,v) + b(w,w)\\
&= b(v,w)+b(w,v)
.\end{align*}
Thus, we conclude that $b$ is skew-symmetric.
\end{proof}

\begin{proposition}
If $k$ does \emph{not} have characteristic $2$, then a bilinear form is
alternating if and only if it is skew-symmetric.
\end{proposition}

\begin{proof}
If $b$ is skew-symmetric, then $b(v,v)=-b(v,v)$ for all $v \in V$.
Thus $2b(v,v)=0$ and, since $\frac{1}{2} \in k$,
we see $b$ is alternating.
\end{proof}

\begin{proposition}
If $k$ has characteristic $2$, then a bilinear form is
symmetric if and only if it is skew-symmetric.
In particular, every alternating form is symmetric.
\end{proposition}

\begin{proof}
Clear.
\end{proof}

\subsection{Sesquilinear Forms}

In this section, we work over the complex numbers.
There are extensions of these ideas to quadratic extensions of other
fields, but we do not want to get too far afield.
Below, if $z=a+bi \in \mathbb{C}$, then we write
$\overline{z}=a-bi$ for its complex conjugate.

\begin{definition}
Let $V$ and $W$ be $\mathbb{C}$-vector spaces.
A \emph{sesquilinear map} $s: V \times W \to \mathbb{C}$,
is a function such that
\begin{itemize}
\item $s(\lambda v_1+v_2,w) = \overline{\lambda} s(v_1,w) + s(v_2,w)$, and
\item $s(v,\lambda w_1+w_2) = \lambda s(v,w_1) + s(v,w_2)$
\end{itemize}
for all $v,v_1,v_2 \in V$, $w,w_1,w_2 \in W$ and $\lambda \in \mathbb{C}$.
\end{definition}

A sesquilinear map is \emph{not} a bilinear form as it is conjugate
linear in the first entry rather than linear.
We use the ``physics convention'' where the first entry is conjugate
linear rather than the second.  This disagrees with many areas of
mathematics, which decrees that the \emph{second} entry is conjugate linear.
The former is much more natural in view of the standard matrix
conventions and seems to ``winning'' in more modern texts.

\begin{definition}
A \emph{sesquilinear form} is a sesquilinear map $s: V \times V \to
\mathbb{C}$.
A sesquilinear form is \emph{hermitian} if $s(v,w)=\overline{s(w,v)}$
for all $v,w \in V$.
A hermitian form is \emph{positive definite} if $s(v,v) > 0$ if and only
if $v \ne 0$.
\end{definition}

Informally, if we restrict a sequilinear form to a vector space over a totally real
subfield $k \subseteq \mathbb{R}$, then hermitian restricts to symmetric
and positive definite restricts to non-degenerate.

\subsection{Classical Groups}

TODO

\subsection{Multilinear Maps}

\begin{definition}
Let $V_1,\ldots, V_d$ be $k$-vector spaces.
A \emph{multilinear map} $m: V_1 \times \cdots \times V_d \to k$,
is a function that is linear in each entry; in other words,
\begin{align*}
&m(v_1,\ldots,v_{i-1},\lambda v_i + v_i',v_{i+1},\ldots,v_d)\\
=& \lambda m(v_1,\ldots,v_{i-1},v_i,v_{i+1},\ldots,v_d)
+ m(v_1,\ldots,v_{i-1},v_i',v_{i+1},\ldots,v_d)
\end{align*}
for ever index $i$ and all $v_j,v_j' \in V_j$, $\lambda \in k$.
A \emph{multilinear form} is a multilinear map where $V_1 = \cdots = V_d$.
\end{definition}

\begin{definition}
A multilinear form is
\emph{symmetric} if
\[
m(\cdots,v_i,\cdots,v_j,\cdots) = m(\cdots,v_j,\cdots,v_i,\cdots)
\]
for all pairs of indices $i,j$.
\end{definition}

\begin{definition}
A multilinear form is \emph{skew-symmetric} or \emph{antisymmetric} if
\[
m(\cdots,v_i,\cdots,v_j,\cdots) = -m(\cdots,v_j,\cdots,v_i,\cdots)
\]
for all distinct pairs of indices $i,j$.
\end{definition}

\begin{definition}
A multilinear form is \emph{alternating} if
\[
m(v_1,\ldots,v_d) = 0
\]
whenever $v_i=v_j$ for some $i \ne j$.
\end{definition}

As in the bilinear case, multilinear maps form a vector space and the
symmetric, skew-symmetric, and alternating forms are subspaces of the
space of multilinear forms.

\begin{exercise}
Prove that
\begin{itemize}
\item alternating forms are skew-symmetric,
\item if $\operatorname{char}(k) \ne 2$, then skew-symmetric forms are
alternating, and
\item if $\operatorname{char}(k) = 2$, then skew-symmetric forms are
symmetric and conversely.
\end{itemize}
\end{exercise}

\section{Tensor Products}

We direct the reader to \cite[10.4]{DF} or \cite[XVI]{Lang}
for more details about the properties of tensor products in the general
module-theoretic setting.
Here we focus on the case of finite-dimensional vector spaces,
which is somewhat easier.

We define tensor products in terms of their universal mapping property.

\begin{definition}
Let $V_1,\ldots, V_n$ be vector spaces over $k$.
A \emph{tensor product} $V_1 \otimes_k V_2 \otimes \cdots \otimes_k V_d$
is a $k$-vector space along with a multilinear map
\[
\pi : V_1 \times \cdots \times V_d
\to  V_1 \otimes_k \cdots \otimes_k V_d
\]
such that for any multilinear map $\phi : V_1 \times \cdots \times V_d
\to W$ there exists a unique linear map
$\psi : V_1 \otimes_k \cdots \otimes_k V_d \to W$ such that
$\phi = \psi \circ \pi$.
\end{definition}

When the base field $k$ is clear, we will often omit it from the
notation; in other words $V \otimes U$ is shorthand for $V \otimes_k U$.
We may also use the shorthand notation
\[
\bigotimes_{i=1}^d V_i := V_1 \otimes \cdots \otimes V_d .
\]

Given $(v_1,\ldots, v_d) \in V_1 \times \cdots \times V_d$,
let $v_1 \otimes v_2 \otimes \cdots \otimes v_d$ denote the image
$\pi(v_1,\ldots,v_d)$ in $V_1 \otimes_k \cdots \otimes_k V_d$.
We call these elements \emph{simple tensors}.

\begin{theorem}
Tensor products exist and are unique up to unique isomorphism.
\end{theorem}

\begin{proof}
We prove existence by construction.
Let $\Omega$ be the free $k$-vector space with basis given by the elements of
$V_1 \times \cdots \times V_d$.  Let $[x]$ be the basis element
corresponding to $x$.
Let $I$ be the subspace of $\Omega$ spanned by
\[
[(v_1, \ldots, (\lambda v_i + v'_i), \ldots, v_d)]
-
\lambda[(v_1, \ldots,v_i, \ldots, v_d)] -
[(v_1, \ldots, v'_i, \ldots, v_d)]
\]
for every index $i$, every $\lambda \in k$ and all $v_j,v'_j \in V_j$
for every $j$.
Define $V_1 \otimes \cdots \otimes V_n$ as the quotient space
$\Omega/I$ with $\pi$ the composition of the canonical inclusion
$\prod_{i=1}^d V_i \to \Omega$ and the quotient $\Omega \to
\bigotimes_{i=1}^d V_i$.

We leave the remaining conditions as an exercise.
\end{proof}

We have the following corollaries:

\begin{corollary}
The vector space $V_1 \otimes_k \cdots \otimes_k V_d$
is spanned by its simple tensors.
The relation
\begin{align*}
&v_1 \otimes \cdots \otimes (\lambda v_i + v'_i) \otimes \cdots \otimes
v_d\\
=&
\lambda(v_1 \otimes \cdots \otimes v_i \otimes \cdots \otimes v_d) +
v_1 \otimes \cdots \otimes v'_i \otimes \cdots \otimes v_d
\end{align*}
holds
for every index $i$, every $\lambda \in k$ and all $v_j,v'_j \in V_j$.
\end{corollary}

\begin{proof}
Exercise.
\end{proof}

\begin{proposition}
If $B_1,\ldots B_d$ are bases for $V_1,\ldots,V_d$, then
\[
\{ b_1 \otimes \cdots \otimes b_d \mid b_1 \in B_1, \ldots, b_d \in B_d
\}
\]
is a basis for $V_1 \otimes_k \cdots \otimes_k V_d$.
\end{proposition}

\begin{proof}
Exercise.
\end{proof}

\begin{remark}
One frequently defines linear transformations by
``extension by linearity.''
Suppose we have vector spaces $V,W$ and we have a spanning
set $S$ for $V$.  If we define a function $f : S \to W$,
then there is at most one linear transformation $F : V \to W$
extending $f$.  When $S$ is a \emph{basis}, the extension $F$ always exists,
but when $S$ is not linearly independent there is something to check.
In the case of tensor products, the set $S$ is frequently taken to be
the set of simple tensors.
\end{remark}

\begin{theorem}
For a finite-dimensional vector space $V$ and an arbitrary vector space
$W$, we have a canonical isomorphism
\[
V^\vee \otimes W \cong \operatorname{Hom}_k(V,W) .
\]
\end{theorem}

\begin{proof}
Define $\psi : V^\vee \otimes W \cong \operatorname{Hom}_k(V,W)$
by defining
\[
\psi(f \otimes w)(v) = f(v)w
\]
for $f \in V^\vee$, $w \in W$, $v \in V$, and extending by linearity.
(Exercise: check that extension by linearity is valid here!)

We explicitly construct an inverse map $\phi$.
Let $v_1,\ldots, v_d$ be a basis for $V$.
Given a linear map $g : V \to W$, define
\[
\phi(g) = \sum_{i=1}^d v^\vee_i \otimes g(v_i) \ .
\]
(Exercise: check that $\phi \circ \psi$ and $\psi \circ \phi$ are
identities.)
\end{proof}

\begin{exercise}
Identifying $V^\vee \otimes V \cong \operatorname{Hom}_k(V,V)$,
the linear map $T: V^\vee \otimes V \to k$ defined by
\[
T\left( \sum_{i=1}^d f_i \otimes v_i\right)
= \sum_{i=1}^d f_i(v_i)
\]
can be identified with the trace $\operatorname{tr} :
\operatorname{Hom}_k(V,V) \to k$.
\end{exercise}

\begin{corollary}
For finite-dimensional vector spaces $V, W$,
we have a canonical isomorphism
\[
V^\vee \otimes W^\vee \cong \operatorname{Bil}_k(V,W) .
\]
\end{corollary}

\begin{proof}
Both are canonically isomorphic to $\operatorname{Hom}_k(V,W^\vee)$.
\end{proof}

\begin{proposition}
Suppose $U,V,W$ are finite-dimensional vector spaces.
There are canonical isomorphisms as follows:
\begin{enumerate}
\item $V \otimes W \cong W \otimes V$
\item $U \otimes (V \oplus W) \cong (U \otimes V) \oplus (U \otimes W)$
\item $U \otimes V \otimes W \cong U \otimes (V \otimes W)
\cong (U \otimes V) \otimes W$
\item $V \otimes k \cong V$
\item $(V \otimes W)^\vee \cong V\vee \otimes W^\vee$
\item $\operatorname{Hom}(U,\operatorname{Hom}(V,W)
\cong \operatorname{Hom}(U \otimes V, W)$
\end{enumerate}
\end{proposition}

\begin{proof}
We leave these all as exercises.
One helpful hint is that
$(V \otimes W)^\vee = \operatorname{Hom}(V \otimes W,k)$
is canonically isomorphic to $\operatorname{Bil}(V,W)$
by the Universal Mapping Property of tensor products.
\end{proof}

\begin{proposition}
Let $L/k$ be a field extension.
If $V$ is a $k$-vector space, then $V \otimes_k L$ is an $L$-vector space.
If $A$ is a $k$-algebra, then $A \otimes_k L$ is an $L$-algebra.
\end{proposition}

\begin{proof}
The set $V \otimes_k L$ is already an abelian group.
We define scalar multiplication by extending by linearity
the formula $\lambda(v\otimes l)=v \otimes \lambda l$
for $\lambda, l \in L$ and $v \in V$.
To see $A$ is an $L$-algebra, we need to define the ring multiplication.
We see that $(a_1 \otimes l_1)(a_2 \otimes l_2)=a_1a_2 \otimes l_1l_2$.
\end{proof}

\section{Tensor Algebras}

\begin{definition}
A \emph{graded $k$-algebra} is a $k$-algebra $A$ with a direct sum
decomposition
\[
A = \bigoplus_{n \ge 0} A_n
\]
such that $A_n \cdot A_m \subseteq A_{n+m}$ for all integers $n,m$.
An element $f \in A_d$ is said to be \emph{homogeneous of degree $d$}.
\end{definition}

The standard example of a graded $k$-algebra is the polynomial
$k$-algebra $k[x_1,\ldots,x_n]$ graded by degree.

\begin{definition}
Let $V$ be a finite-dimensional $k$-vector space.  For a positive integer
$n$, we define $\mathcal{T}^d(V)=\bigotimes_{i=1}^d V$ and
$\mathcal{T}^0(V)=k$.  We define the \emph{tensor $k$-algebra on $V$}
as the graded $k$-algebra
\[
\mathcal{T}(V) := \bigoplus_{d \ge 0} \mathcal{T}^d(V)
\]
with multiplication
\[\mathcal{T}^d(V) \times \mathcal{T}^e(V)
\to \mathcal{T}^{d+e}(V)
\]
defined via
\[
(v_1 \otimes \cdots \otimes v_d) (w_1 \otimes \cdots \otimes w_e)
=v_1 \otimes \cdots \otimes v_d \otimes w_1 \otimes \cdots \otimes w_e
\]
and extended to all of $\mathcal{T}(V)$ by linearity. 
\end{definition}

\begin{example}
If $V=k^n$, then $\mathcal{T}(V)$ can be identified with
non-commutative polynomial ring $k\langle x_1,\ldots, x_n\rangle$.
For example, if $e_1,\ldots, e_n$ is the standard basis for $k^n$,
then
\[
e_1 \otimes e_1 \otimes e_1 + 3e_1 \otimes e_2 - e_2\otimes e_1 + 4\cdot 1
\]
in $\mathcal{T}(k^n)$
corresponds to the inhomogeneous element
\[
x_1^3 + 3x_1x_2 - x_2x_1 + 4
\]
in $k\langle x_1,\ldots, x_n\rangle$.
\end{example}

The following is less general than descriptions of tensor products
above, but it is useful to have a contrast to the symmetric and exterior
algebras discussed below.

\begin{proposition}
If $e_1, \ldots, e_n$ is a basis for $V$, then
\[
\{
e_{i_1} \otimes e_{i_2} \otimes \cdots \otimes e_{i_d}
\mid
(i_1,\ldots,i_d) \in \{1,\ldots,n\}^d
\}
\]
is a basis for $\mathcal{T}^d(V)$.
In particular, $\dim_k\ \mathcal{T}^d(k^n)=n^d$.
\end{proposition}

\subsection{Symmetric Algebra}

\begin{definition}
Let $V$ be a finite dimensional $k$-vector space.
The \emph{symmetric algebra on $V$} is the graded $k$-algebra
\[
\mathcal{S}(V) = \mathcal{T}(V)/I
\]
where $I$ is the (two-sided) graded ideal
\[
I = ( v \otimes w - w \otimes v \mid w,v \in V ).
\]
The graded piece $\mathcal{S}^d(V)$ is called the
\emph{$d$th symmetric power}.
\end{definition}

Symmetric powers satisfy a universal mapping property building on the
one for tensor products:

\begin{proposition}
The composition
\[
\pi : V^d \to \mathcal{T}^d(V) \to \mathcal{S}^d(V)
\]
is a symmetric multilinear map such that for any
symmetric multilinear map
$\phi : V^d \to W$ there exists a unique linear map
$\psi : \mathcal{S}^d(V) \to W$
such that
$\phi = \psi \circ \pi$.
\end{proposition}

If $V$ is an $n$-dimensional vector space, then
$\mathcal{S}(V) \cong k[x_1,\ldots,x_n]$ as graded $k$-algebras.
Thus, the symmetric algebra is essentially a ``coordinate-free'' version of
the polynomial ring.
For this reason, the multiplication of symmetric algebra is usually just
as monomials in the constituent vectors.
For example, the image of $e_1 \otimes e_2 \otimes e_1$ might be
written $e_1^2e_2$.

We have the following combinatorial proposition.

\begin{proposition}
If $e_1, \ldots, e_n$ is a basis for $V$, then
\[
\{
e_{i_1}e_{i_2} \cdots e_{i_d}
\mid
1 \le i_1 \le \cdots \le i_d \le n
\}
\]
is a basis for $\mathcal{S}^d(V)$.
In particular, $\dim_k\ \mathcal{S}^d(k^n)=\binom{n+d-1}{d}$.
\end{proposition}

\subsection{Exterior Algebra}

\begin{definition}
Let $V$ be a finite dimensional $k$-vector space.
The \emph{exterior algebra on $V$} is the graded $k$-algebra
\[
\Lambda^d(V) = \mathcal{T}(V)/J
\]
where $J$ is the (two-sided) graded ideal
\[
J = ( v \otimes v \mid v \in V ).
\]
The graded piece $\Lambda^d(V)$ is called the
\emph{$d$th exterior power}.
\end{definition}

Exterior powers satisfy a universal mapping property building on the
one for tensor products:

\begin{proposition}
The composition
\[
\pi : V^d \to \mathcal{T}^d(V) \to \Lambda^d(V)
\]
is an alternating multilinear map such that for any
alternating multilinear map
$\phi : V^d \to W$ there exists a unique linear map
$\psi : \Lambda^d(V) \to W$
such that
$\phi = \psi \circ \pi$.
\end{proposition}

The image of a simple tensor $v_1 \otimes v_2 \otimes \cdots \otimes v_d$
is written $v_1 \wedge v_2 \wedge \cdots \wedge v_d$.
Similarly, the multiplication of two elements $f,g \in \Lambda^d(V)$
is written $f \wedge g$.

\begin{proposition}
If $v,w \in V$, then $v \wedge w = -w \wedge v$.
\end{proposition}

\begin{proof}
Follows immediately from
\begin{align*}
0 &= (v+w) \wedge (v+w)\\
&=v \wedge v + v \wedge w + w \wedge v + w \wedge w\\
&=v \wedge w + w \wedge v
\end{align*}
\end{proof}

\begin{proposition}
If $v_1,\ldots,v_d \in V$ and $\sigma \in S_d$,
then
\[v_1 \wedge \cdots \wedge v_d =
\operatorname{sgn}(\sigma)
v_{\sigma(1)} \wedge \cdots \wedge v_{\sigma(d)}.
\]
\end{proposition}

\begin{proof}
Every permutation can be written as a product of adjacent
transpositions and $\operatorname{sgn}(\sigma)$ is the number of such
transpositions.  Thus, the result follows from the previous proposition.
\end{proof}

\begin{proposition}
If $f \in \Lambda^d(V)$ and $g \in \Lambda^e(V)$,
then $f \wedge g = (-1)^{de} g \wedge f$.
\end{proposition}

\begin{proof}
This follows from the previous proposition in view of the fact that
the permutation
\[
(1,\ldots,d+e) \mapsto (d+1,\ldots,d+e,1,\ldots,d)
\]
has sign $(-1)^{de}$.
\end{proof}

\begin{proposition}
Vectors $v_1,\ldots , v_d \in V$ are linearly dependent
if and only if $v_1 \wedge \cdots \wedge v_d=0$.
\end{proposition}

\begin{proof}
Suppose $v_1,\ldots, v_d$ are linearly dependent.
Without loss of generality, we may assume
\[
v_d = \alpha_1 v_1 + \cdots + \alpha_{d-1} v_{d-1}
\]
where $\alpha_1, \ldots, \alpha_{d-1} \in k$.
Thus
\[
v_1 \wedge \cdots \wedge v_d =
\sum_{i=1}^{d-1} \alpha_i(v_1 \wedge \vdots \wedge v_{d-1} \wedge v_i).
\]
Since $v_i$ occurs twice in each summand, the right hand side is $0$.
Now, suppose $v_1,\ldots,v_d$ are linearly independent.

Suppose $v_1,\ldots, v_d$ are linearly independent.
We can complete to a basis $v_1,\ldots,v_n$ for $V$.
Let $\beta : V \to k^n$ be the corresponding coordinate map.
Let $f : V^d \to k$ be the function
\[
f(w_1,\ldots,w_d) = \det\left(
\begin{pmatrix} \beta(w_1) & \cdots & \beta(w_d)
& e_{d+1} & \cdots & e_n
\end{pmatrix}
\right) .
\]
Since the determinant is an alternating multilinear map on the columns
of a square matrix, we see that $f$ is an alternating multilinear map
such that $f(v_1,\ldots,v_d)=1$.
By the universal mapping property, if there exists an altenating
multilinear map such that $f(v_1,\ldots,v_d) \ne 1$,
then $v_1 \wedge \cdots \wedge v_d$ must be non-zero.
\end{proof}

We have the following combinatorial proposition.

\begin{proposition}
If $e_1, \ldots, e_n$ is a basis for $V$, then
\[
\{
e_{i_1} \wedge \cdots e_{i_2} \cdots \wedge e_{i_d}
\mid
1 \le i_1 < \cdots < i_d \le n
\}
\]
is a basis for $\Lambda^d(V)$.
In particular, $\dim_k\ \Lambda^d(k^n)=\binom{n}{d}$.
\end{proposition}

\subsection{Subspaces of Tensor Powers}

There is a natural left action of the symmetric group $S_d$ on the
tensor power $\mathcal{T}^d(V)$ given by
\[
\sigma(v_1 \otimes v_2 \otimes \cdots \otimes v_n)
= v_{\sigma^{-1}(1)} \otimes v_{\sigma^{-1}(2)} \otimes \cdots
\otimes v_{\sigma^{-1}(n)}
\]
for $\sigma \in S_d$.
The inverse is important to guarantee that it is a \emph{left} action,
but this will not be especially relevant in what follows.

\begin{definition}
Let $V$ be a vector space on $k$.
The \emph{space of symmetric $d$-tensors on $V$} is
\[
\operatorname{Sym}^d(V) = \operatorname{span}_k
\left\{ t \in \mathcal{T}^d(V) \mid \sigma(t)=t
\textrm{ for all } t \in S_d  \right\} .
\]
The \emph{space of alternating $d$-tensors on $V$} is
\[
\operatorname{Alt}^d(V) = \operatorname{span}_k
\left\{ t \in \mathcal{T}^d(V) \mid
\sigma(t)=\operatorname{sgn}(\sigma)t
\textrm{ for all } t \in S_d  \right\} .
\]
\end{definition}

Note that while $\mathcal{S}^d(V)$ and $\Lambda^d(V)$ are
\emph{quotients} of $\mathcal{T}^d(V)$,
the spaces $\operatorname{Sym}^d(V)$
and $\operatorname{Alt}^d(V)$
are \emph{subspaces}.

\begin{example}
We have
\[
e_1 \otimes e_2 \otimes e_2 + e_2 \otimes e_1 \otimes e_2
+ e_2 \otimes e_2 \otimes e_1 \in \operatorname{Sym}^d(k^5)
\]
and
\[
3\left(e_1 \otimes e_2 - e_2 \otimes e_1\right)
+ 4\left( e_3 \otimes e_5 - e_5 \otimes e_3 \right) 
\in \operatorname{Sym}^d(k^7).
\]
Observe that the symmetric group does \emph{not} act on the
vector space $V$.
\end{example}

\begin{proposition}
Suppose $\operatorname{char}(k) > d$.  Then the compositions
\begin{align*}
\operatorname{Sym}^d(V) &\to \mathcal{T}^d(V) \to \mathcal{S}^d(V)\\
\operatorname{Alt}^d(V) &\to \mathcal{T}^d(V) \to \Lambda^d(V)\\
\end{align*}
are isomorphisms.
\end{proposition}

\begin{proof}
We define a \emph{polarization} map
$P : \mathcal{S}^d(V) \to \operatorname{Sym}^d(V)$ as follows.
Consider first the map $p : \mathcal{T}^d \to \mathcal{T}^d$
defined via
\[
p(v_1 \otimes v_2 \otimes \cdots \otimes v_d)
= \frac{1}{d!}\sum_{\sigma \in S_d}
v_{\sigma(1)} \otimes v_{\sigma(1)} \otimes \cdots \otimes v_{\sigma(d)}
.
\]
Observe that $p$ is a linear projection onto $\operatorname{Sym}^d(V)$
with kernel precisely $I \cap \mathcal{T}^d(V)$ from the definition
of $\mathcal{S}^d(V)$.  Thus we obtain the desired map $P$.
One checks that $P$ is an inverse of the composition in the statement of
the proposition.

There is a similar construction for $\operatorname{Alt}^d(V)$
and $\Lambda^d(V)$.
\end{proof}


%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%

\bibliographystyle{alpha}
\bibliography{rep_theory}

\end{document}


